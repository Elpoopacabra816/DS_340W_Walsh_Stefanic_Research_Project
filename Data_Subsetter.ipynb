{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO45C3NqXcAbWgTpABhwgPT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ======================================================\n","# CTC per-source balanced subsetter (Colab 1-click)\n","# - Pulls from Reddit, Stackexchange, arXiv in Zenodo tar\n","# - 10,000 per source (5k non-cyber + 5k cyber) by default\n","# - Saves one file per source + one combined file (with \"source\")\n","# ======================================================\n","\n","# -------- CONFIG --------\n","PER_SOURCE_TOTAL = 20_000      # total per source\n","BALANCED_PER_SOURCE = True     # keep 50/50 cyber vs non per source\n","USE_GOOGLE_DRIVE = True\n","OUT_DIR = \"/content/drive/MyDrive/CTC_by_source\"  # folder for outputs (Drive or local)\n","RANDOM_SEED = 1337\n","\n","ZENODO_URL = \"https://zenodo.org/records/10655913/files/CTC_training_data.tar.gz?download=1\"\n","\n","# File names\n","REDDIT_OUT = \"CTC_Reddit_10k.json\"\n","STACK_OUT  = \"CTC_Stackexchange_10k.json\"\n","ARXIV_OUT  = \"CTC_arXiv_10k.json\"\n","COMBINED_OUT = \"CTC_by_source_30k.json\"\n","\n","# -------- Installs --------\n","import sys, subprocess, importlib.util\n","def _pip(pkgs): subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs, check=True)\n","need = [p for p in [\"ijson\",\"requests\",\"tqdm\"] if importlib.util.find_spec(p) is None]\n","if need: _pip(need)\n","\n","# -------- Imports --------\n","import json, re, tarfile, random, posixpath\n","from pathlib import Path\n","from typing import Dict, List, Optional, Tuple\n","import ijson, requests\n","from tqdm import tqdm\n","\n","# -------- Mount Drive (optional) --------\n","if USE_GOOGLE_DRIVE:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","\n","# -------- Helpers --------\n","def detect_source_and_label(member_name: str) -> Tuple[Optional[str], Optional[int]]:\n","    \"\"\"\n","    From a tar member path like:\n","      CTC_training_data_text_export/Reddit/not_cybersecurity.json\n","      CTC_training_data_text_export/Stackexchange/cybersecurity.json\n","      CTC_training_data_text_export/arXiv/not_cybersecurity.json\n","    return (source, label) where source in {\"Reddit\",\"Stackexchange\",\"arXiv\"}\n","    and label in {0 (non-cyber), 1 (cyber)}.\n","    \"\"\"\n","    n = member_name.strip()\n","    parts = n.split(\"/\")\n","    # Expect .../<Source>/<file>.json\n","    source = None\n","    if len(parts) >= 3:\n","        source = parts[-2]  # Reddit | Stackexchange | arXiv (case-sensitive in Zenodo)\n","    fname = parts[-1].lower()\n","\n","    # label from filename\n","    if re.search(r'(?:^|/)(?:not|non)[-_]?cybersecurity\\.json$', fname, re.I):\n","        label = 0\n","    elif re.search(r'(?:^|/)cybersecurity\\.json$', fname, re.I):\n","        label = 1\n","    else:\n","        # fallback\n","        if \"cyber\" in fname and (\"not_\" in fname or \"not-\" in fname or \"non_\" in fname or \"non-\" in fname):\n","            label = 0\n","        elif \"cyber\" in fname:\n","            label = 1\n","        else:\n","            label = None\n","    return source, label\n","\n","def reservoir_insert(reservoir: List[dict], k: int, item: dict, seen: int, rng: random.Random):\n","    if len(reservoir) < k:\n","        reservoir.append(item)\n","    else:\n","        j = rng.randint(1, seen)  # inclusive\n","        if j <= k:\n","            reservoir[j - 1] = item\n","\n","def write_json_array(path: Path, items: List[dict]):\n","    path.parent.mkdir(parents=True, exist_ok=True)\n","    with path.open(\"w\", encoding=\"utf-8\") as f:\n","        f.write(\"[\\n\")\n","        for i, obj in enumerate(items):\n","            if i:\n","                f.write(\",\\n\")\n","            json.dump(obj, f, ensure_ascii=False)\n","        f.write(\"\\n]\\n\")\n","\n","def make_per_source_balanced(\n","    url: str,\n","    out_dir: Path,\n","    per_source_total: int = 10_000,\n","    balanced: bool = True,\n","    seed: int = 1337\n",") -> Dict:\n","    \"\"\"\n","    Build per-source reservoirs:\n","      sources = [\"Reddit\",\"Stackexchange\",\"arXiv\"]\n","      If balanced=True, keep per_source_total/2 per class (0 and 1) per source.\n","      Otherwise, keep per_source_total with reservoir across both classes (still labeled).\n","    Stop early once all per-source quotas are filled.\n","    Returns diagnostics.\n","    \"\"\"\n","    rng = random.Random(seed)\n","    sources = [\"Reddit\", \"Stackexchange\", \"arXiv\"]\n","\n","    # targets\n","    if balanced:\n","        k_per_class = per_source_total // 2\n","        targets = {s: {0: k_per_class, 1: k_per_class} for s in sources}\n","    else:\n","        # not used here, but left for completeness (single reservoir per source)\n","        targets = {s: {0: per_source_total, 1: 0} for s in sources}\n","\n","    # reservoirs & counters\n","    res = {s: {0: [], 1: []} for s in sources}\n","    seen = {s: {0: 0, 1: 0} for s in sources}\n","\n","    diags = {\n","        \"members_seen\": 0,\n","        \"items_seen_total\": 0,\n","        \"per_source_used\": {s: {0: 0, 1: 0} for s in sources},\n","        \"per_source_seen\": {s: {0: 0, 1: 0} for s in sources},\n","        \"members_skipped\": 0\n","    }\n","\n","    def all_filled():\n","        for s in sources:\n","            for c in (0, 1):\n","                if len(res[s][c]) < targets[s][c]:\n","                    return False\n","        return True\n","\n","    with requests.get(url, stream=True) as r:\n","        r.raise_for_status()\n","        tf = tarfile.open(fileobj=r.raw, mode=\"r|gz\")\n","        for member in tf:\n","            if not member.name.endswith(\".json\"):\n","                continue\n","            diags[\"members_seen\"] += 1\n","\n","            source, label = detect_source_and_label(member.name)\n","            if source not in sources or label not in (0, 1):\n","                diags[\"members_skipped\"] += 1\n","                continue\n","\n","            fobj = tf.extractfile(member)\n","            if fobj is None:\n","                diags[\"members_skipped\"] += 1\n","                continue\n","\n","            need = targets[source][label]\n","            # If this source/class is already full, skip reading this member fast\n","            if len(res[source][label]) >= need:\n","                continue\n","\n","            pbar = tqdm(total=None, unit=\"items\",\n","                        desc=f\"Streaming {member.name} -> {source}, label={label}\")\n","            for item in ijson.items(fobj, \"item\"):\n","                diags[\"items_seen_total\"] += 1\n","                if not isinstance(item, str):\n","                    continue\n","                diags[\"per_source_seen\"][source][label] += 1\n","                seen[source][label] += 1\n","\n","                if len(res[source][label]) < need:\n","                    obj = {\"text\": item, \"label\": label, \"source\": source}\n","                    reservoir_insert(res[source][label], need, obj, seen[source][label], rng)\n","\n","                pbar.update(1)\n","                # If this source/class just became full, we can consider breaking early only if all are full.\n","                if all_filled():\n","                    pbar.close()\n","                    break\n","            else:\n","                pbar.close()\n","\n","            if all_filled():\n","                break\n","\n","    # write per-source files\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","    paths = {}\n","    for s in sources:\n","        items = res[s][0] + res[s][1]\n","        rng.shuffle(items)\n","        fname = {\n","            \"Reddit\": \"CTC_Reddit_10k.json\",\n","            \"Stackexchange\": \"CTC_Stackexchange_10k.json\",\n","            \"arXiv\": \"CTC_arXiv_10k.json\"\n","        }[s]\n","        p = out_dir / fname\n","        write_json_array(p, items)\n","        paths[s] = str(p)\n","        diags[\"per_source_used\"][s][0] = len(res[s][0])\n","        diags[\"per_source_used\"][s][1] = len(res[s][1])\n","\n","    # write combined file\n","    combined = []\n","    for s in sources:\n","        combined.extend(res[s][0])\n","        combined.extend(res[s][1])\n","    rng.shuffle(combined)\n","    combined_path = out_dir / \"CTC_by_source_30k.json\"\n","    write_json_array(combined_path, combined)\n","    paths[\"combined\"] = str(combined_path)\n","\n","    return {\"paths\": paths, \"diags\": diags}\n","\n","# -------- Run it --------\n","out_dir = Path(OUT_DIR)\n","result = make_per_source_balanced(\n","    url=ZENODO_URL,\n","    out_dir=out_dir,\n","    per_source_total=PER_SOURCE_TOTAL,\n","    balanced=BALANCED_PER_SOURCE,\n","    seed=RANDOM_SEED\n",")\n","\n","print(\"\\n✅ Done.\")\n","print(\"Files written:\")\n","for k, v in result[\"paths\"].items():\n","    print(f\"  {k}: {v}\")\n","\n","print(\"\\nDiagnostics:\")\n","print(json.dumps(result[\"diags\"], indent=2))\n","\n","# quick sanity\n","from collections import Counter\n","for s, p in result[\"paths\"].items():\n","    if s == \"combined\": continue\n","    data = json.loads(Path(p).read_text(encoding=\"utf-8\"))\n","    print(f\"\\n{s}: loaded {len(data)}\")\n","    print(\"label counts:\", Counter(d[\"label\"] for d in data))\n","    print(\"sample:\", (data[0][\"text\"][:120] + (\"…\" if len(data[0][\"text\"])>120 else \"\")))\n","# combined\n","cp = result[\"paths\"][\"combined\"]\n","cdata = json.loads(Path(cp).read_text(encoding=\"utf-8\"))\n","print(f\"\\ncombined: loaded {len(cdata)}\")\n","print(\"label counts:\", Counter(d[\"label\"] for d in cdata))\n","print(\"source counts:\", Counter(d[\"source\"] for d in cdata))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zgBLDknCL9g4","outputId":"70c87af2-a838-4c5b-dfc1-10eb308aa84b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["Streaming CTC_training_data_text_export/Reddit/not_cybersecurity.json -> Reddit, label=0: 4184184items [00:34, 121534.35items/s]\n","Streaming CTC_training_data_text_export/Reddit/cybersecurity.json -> Reddit, label=1: 164750items [00:01, 137964.91items/s]\n","Streaming CTC_training_data_text_export/arXiv/not_cybersecurity.json -> arXiv, label=0: 28996items [00:25, 1117.91items/s]\n","Streaming CTC_training_data_text_export/arXiv/cybersecurity.json -> arXiv, label=1: 12132items [00:13, 906.16items/s]\n","Streaming CTC_training_data_text_export/Stackexchange/not_cybersecurity.json -> Stackexchange, label=0: 4842461items [01:57, 41220.27items/s] \n","Streaming CTC_training_data_text_export/Stackexchange/cybersecurity.json -> Stackexchange, label=1: 10000items [00:00, 60432.91items/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","✅ Done.\n","Files written:\n","  Reddit: /content/drive/MyDrive/CTC_by_source/CTC_Reddit_10k.json\n","  Stackexchange: /content/drive/MyDrive/CTC_by_source/CTC_Stackexchange_10k.json\n","  arXiv: /content/drive/MyDrive/CTC_by_source/CTC_arXiv_10k.json\n","  combined: /content/drive/MyDrive/CTC_by_source/CTC_by_source_30k.json\n","\n","Diagnostics:\n","{\n","  \"members_seen\": 6,\n","  \"items_seen_total\": 9242523,\n","  \"per_source_used\": {\n","    \"Reddit\": {\n","      \"0\": 10000,\n","      \"1\": 10000\n","    },\n","    \"Stackexchange\": {\n","      \"0\": 10000,\n","      \"1\": 10000\n","    },\n","    \"arXiv\": {\n","      \"0\": 10000,\n","      \"1\": 10000\n","    }\n","  },\n","  \"per_source_seen\": {\n","    \"Reddit\": {\n","      \"0\": 4184184,\n","      \"1\": 164750\n","    },\n","    \"Stackexchange\": {\n","      \"0\": 4842461,\n","      \"1\": 10000\n","    },\n","    \"arXiv\": {\n","      \"0\": 28996,\n","      \"1\": 12132\n","    }\n","  },\n","  \"members_skipped\": 0\n","}\n","\n","Reddit: loaded 20000\n","label counts: Counter({0: 10000, 1: 10000})\n","sample: Look both ways before crossing... ok, at least look one way... no, not that way.... \n","\n","Stackexchange: loaded 20000\n","label counts: Counter({1: 10000, 0: 10000})\n","sample: What is the most successful virus/rootkit?. <p>\"Successful\" is rated by infection rate. </p>\n","\n","<p>Which virus/rootkit/mal…\n"]}]},{"cell_type":"code","source":["def prepare_splits_for_texts(X, y):\n","    X_train_text, X_test_text, y_train, y_test = train_test_split(\n","        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n","    )\n","    X_train_text, X_val_text, y_train, y_val = train_test_split(\n","        X_train_text, y_train, test_size=VAL_SIZE_WITHIN_TRAIN, random_state=RANDOM_STATE, stratify=y_train\n","    )\n","    return X_train_text, y_train, X_val_text, y_val, X_test_text, y_test\n","\n","def bench_source(name: str, X: List[str], y: List[int]):\n","    print(f\"\\n===== {name}: {len(X)} samples =====\")\n","    X_train_text, y_train, X_val_text, y_val, X_test_text, y_test = prepare_splits_for_texts(X, y)\n","    results = []\n","    for method in METHODS.keys():\n","        print(f\"\\n[{name}] Method: {method}\")\n","        acc, report, timings = train_eval_method(X_train_text, y_train, X_val_text, y_val, X_test_text, y_test, method)\n","        results.append({\"source\": name, \"method\": method, \"accuracy\": acc, **timings})\n","        print(f\"  accuracy={acc:.4f}\")\n","    return pd.DataFrame(results)\n","\n","# Load per-source data\n","Xr, yr = load_ctc_json(RED_PATH)\n","Xs, ys = load_ctc_json(STK_PATH)\n","Xa, ya = load_ctc_json(ARX_PATH)\n","\n","df_r = bench_source(\"Reddit\",       Xr, yr)\n","df_s = bench_source(\"Stackexchange\", Xs, ys)\n","df_a = bench_source(\"arXiv\",        Xa, ya)\n","\n","df_all = pd.concat([df_r, df_s, df_a], ignore_index=True)\n","df_all.to_csv(WORKDIR / \"vectorizer_bench_results.csv\", index=False)\n","df_all.head()"],"metadata":{"id":"fYpQzjYLXQE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_accuracy_bars(df, title):\n","    order = df.groupby(\"method\")[\"accuracy\"].mean().sort_values(ascending=False).index.tolist()\n","    plt.figure(figsize=(10,5))\n","    plt.bar(df[\"method\"], df[\"accuracy\"])\n","    plt.xticks(rotation=60, ha=\"right\")\n","    plt.ylim(0,1)\n","    plt.title(title)\n","    plt.ylabel(\"Accuracy\")\n","    plt.grid(axis=\"y\", alpha=0.3)\n","    plt.show()\n","\n","def plot_accuracy_per_source(df_all):\n","    for src in [\"Reddit\",\"Stackexchange\",\"arXiv\"]:\n","        df = df_all[df_all[\"source\"]==src].copy()\n","        df = df.sort_values(\"accuracy\", ascending=False)\n","        plt.figure(figsize=(10,5))\n","        plt.bar(df[\"method\"], df[\"accuracy\"])\n","        plt.xticks(rotation=60, ha=\"right\"); plt.ylim(0,1)\n","        plt.title(f\"{src} — Accuracy by method\")\n","        plt.ylabel(\"Accuracy\"); plt.grid(axis=\"y\", alpha=0.3)\n","        plt.show()\n","\n","def plot_timing(df_all, timing_key: str, title: str):\n","    for src in [\"Reddit\",\"Stackexchange\",\"arXiv\"]:\n","        df = df_all[df_all[\"source\"]==src].copy()\n","        if timing_key not in df.columns: continue\n","        df = df.sort_values(timing_key, ascending=True)\n","        plt.figure(figsize=(10,5))\n","        plt.bar(df[\"method\"], df[timing_key])\n","        plt.xticks(rotation=60, ha=\"right\")\n","        plt.title(f\"{src} — {title}\")\n","        plt.ylabel(\"Seconds\"); plt.grid(axis=\"y\", alpha=0.3)\n","        plt.show()\n","\n","plot_accuracy_per_source(df_all)\n","plot_timing(df_all, \"fit_vectorizer_s\", \"Vectorizer/Embedder Fit Time (s)\")\n","plot_timing(df_all, \"transform_s\", \"Transform/Embed Time (s)\")\n","plot_timing(df_all, \"fit_clf_s\", \"Classifier Fit Time (s)\")\n","plot_timing(df_all, \"infer_s\", \"Inference Time (s)\")"],"metadata":{"id":"XvmZEpAdXVgK"},"execution_count":null,"outputs":[]}]}