{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_DcwaDMNOmn-"
      },
      "outputs": [],
      "source": [
        "import os, random, numpy as np\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "random.seed(42); np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Install ====\n",
        "import sys, subprocess, importlib.util\n",
        "def pip_install(pkgs): subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs, check=True)\n",
        "need = []\n",
        "for p in [\"numpy\",\"pandas\",\"scikit-learn\",\"joblib\",\"tqdm\",\"requests\",\"matplotlib\",\n",
        "          \"sentence-transformers\",\"transformers\",\"torch\",\"ijson\"]:\n",
        "    if importlib.util.find_spec(p) is None:\n",
        "        need.append(p)\n",
        "if need: pip_install(need)\n",
        "\n",
        "# ==== Imports ====\n",
        "import os, json, re, time, zipfile\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Callable, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests, joblib\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# ==== Paths & knobs ====\n",
        "USE_GOOGLE_DRIVE = True\n",
        "BASE = Path(\"/content/drive/MyDrive/CTC_by_source\") if USE_GOOGLE_DRIVE else Path(\"/content\")\n",
        "RED_PATH = BASE / \"CTC_Reddit_10k.json\"\n",
        "STK_PATH = BASE / \"CTC_Stackexchange_10k.json\"\n",
        "ARX_PATH = BASE / \"CTC_arXiv_10k.json\"\n",
        "\n",
        "WORKDIR = Path(\"ctc_bench\"); WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "CACHE = WORKDIR / \"cache\"; CACHE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE_WITHIN_TRAIN = 0.125    # 10% of original becomes validation (0.8 * 0.125)\n",
        "RANDOM_STATE = 42\n",
        "BATCH_SIZE_TXT = 512\n",
        "\n",
        "# Authors' repo (dictionary + validation)\n",
        "CTC_REPO_ZIP = \"https://codeload.github.com/epelofske-student/CTC/zip/refs/heads/main\"\n",
        "DICT_REPO_PATH = \"English_word_dictionary.txt\"\n",
        "VAL_DIR_CYB = \"validation_data_cybersecurity\"\n",
        "VAL_DIR_NON = \"validation_data_non_cybersecurity\"\n",
        "DATA = WORKDIR / \"data\"; DATA.mkdir(exist_ok=True, parents=True)"
      ],
      "metadata": {
        "id": "CXqCHHGqWxQU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_download(url: str, out_path: Path, desc: str = None):\n",
        "    if out_path.exists() and out_path.stat().st_size > 0:\n",
        "        return out_path\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        total = int(r.headers.get(\"content-length\", 0))\n",
        "        with open(out_path, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=desc or out_path.name) as pbar:\n",
        "            for chunk in r.iter_content(1024*1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk); pbar.update(len(chunk))\n",
        "    return out_path\n",
        "\n",
        "# Get authors' repo as ZIP\n",
        "repo_zip = DATA / \"CTC-main.zip\"\n",
        "repo_root = DATA / \"CTC-main\"\n",
        "if not repo_root.exists():\n",
        "    stream_download(CTC_REPO_ZIP, repo_zip, desc=\"CTC-main.zip\")\n",
        "    with zipfile.ZipFile(repo_zip, \"r\") as z:\n",
        "        z.extractall(DATA)\n",
        "DICT_PATH = repo_root / DICT_REPO_PATH\n",
        "assert DICT_PATH.exists(), \"Dictionary not found in repo\"\n",
        "\n",
        "# Cleaning\n",
        "CLEAN_HTML_RE = re.compile(r\"<[^>]+>\")\n",
        "URL_RE = re.compile(r\"http[s]?://\\S+|www\\.\\S+\")\n",
        "CODE_RE = re.compile(r\"`{1,3}.*?`{1,3}\", re.DOTALL)\n",
        "NON_ASCII_RE = re.compile(r\"[^\\x00-\\x7F]+\")\n",
        "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = URL_RE.sub(\" \", s)\n",
        "    s = CODE_RE.sub(\" \", s)\n",
        "    s = CLEAN_HTML_RE.sub(\" \", s)\n",
        "    s = NON_ASCII_RE.sub(\" \", s)\n",
        "    s = WHITESPACE_RE.sub(\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def load_ctc_json(path: Path):\n",
        "    data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "    X = [clean_text(d[\"text\"]) for d in data]\n",
        "    y = [int(d[\"label\"]) for d in data]\n",
        "    return X, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uOPW1p0W2Xb",
        "outputId": "251b8fea-1317-41bc-fa70-c6f2b1b077ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CTC-main.zip: 49.3MB [00:02, 16.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed dictionary TF-IDF (paper baseline)\n",
        "def make_tfidf_dict():\n",
        "    vocab = sorted({w.strip() for w in DICT_PATH.read_text(\"utf-8\").splitlines() if w.strip()})\n",
        "    return TfidfVectorizer(vocabulary=vocab, lowercase=True, dtype=np.float32,\n",
        "                           token_pattern=r\"(?u)\\b\\w+\\b\", norm=\"l2\", use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
        "\n",
        "# Free vocab TF-IDF\n",
        "def make_tfidf_free():\n",
        "    return TfidfVectorizer(min_df=2, ngram_range=(1,2), lowercase=True, dtype=np.float32)\n",
        "\n",
        "# Char n-gram TF-IDF\n",
        "def make_tfidf_char():\n",
        "    return TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), lowercase=True, dtype=np.float32)\n",
        "\n",
        "# CountVectorizer\n",
        "def make_count():\n",
        "    return CountVectorizer(min_df=2, ngram_range=(1,2), lowercase=True, dtype=np.int32)\n",
        "\n",
        "# HashingVectorizer (stateless)\n",
        "def make_hashing():\n",
        "    return HashingVectorizer(n_features=2**18, alternate_sign=False, norm=\"l2\", lowercase=True)\n",
        "\n",
        "# LSA  (TF-IDF -> SVD(256) -> l2-normalize)\n",
        "def make_lsa():\n",
        "    tfidf = TfidfVectorizer(min_df=2, ngram_range=(1,2), lowercase=True, dtype=np.float32)\n",
        "    svd = TruncatedSVD(n_components=256, random_state=RANDOM_STATE)\n",
        "    norm = Normalizer(copy=False)\n",
        "    return make_pipeline(tfidf, svd, norm)\n",
        "\n",
        "# Sentence-Transformers\n",
        "_SMODELS = {\n",
        "    \"sbert_all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"e5-small-v2\":            \"intfloat/e5-small-v2\",\n",
        "    \"bge-small-en-v1.5\":      \"BAAI/bge-small-en-v1.5\",\n",
        "}\n",
        "\n",
        "def embed_with_sbert(model_name: str, texts: List[str], batch_size: int = 128, device: str = None) -> np.ndarray:\n",
        "    m = SentenceTransformer(_SMODELS[model_name], device=device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    emb = m.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return emb.astype(\"float32\")\n",
        "\n",
        "# Vanilla Transformers CLS (e.g., distilroberta-base)\n",
        "def embed_with_transformer_cls(hf_name: str, texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tok = AutoTokenizer.from_pretrained(hf_name)\n",
        "    mdl = AutoModel.from_pretrained(hf_name).to(device)\n",
        "    mdl.eval()\n",
        "    outs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Encoding {hf_name} CLS\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tok(batch, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            out = mdl(**enc).last_hidden_state[:,0,:]  # CLS token\n",
        "            out = torch.nn.functional.normalize(out, p=2, dim=1)\n",
        "        outs.append(out.detach().cpu().numpy().astype(\"float32\"))\n",
        "    return np.vstack(outs)\n",
        "\n",
        "# Registry of 10 methods\n",
        "METHODS = {\n",
        "    # Classic 6\n",
        "    \"tfidf_dict_word\":   (\"sparse\", make_tfidf_dict),\n",
        "    \"tfidf_free_word\":   (\"sparse\", make_tfidf_free),\n",
        "    \"tfidf_char_3_5\":    (\"sparse\", make_tfidf_char),\n",
        "    \"count_word\":        (\"sparse\", make_count),\n",
        "    \"hashing_word\":      (\"sparse\", make_hashing),\n",
        "    \"lsa_tfidf_svd256\":  (\"dense\",  make_lsa),\n",
        "    # Neural / LLM 4\n",
        "    \"sbert_all-MiniLM-L6-v2\": (\"dense_embed\", lambda: \"sbert_all-MiniLM-L6-v2\"),\n",
        "    \"e5-small-v2\":            (\"dense_embed\", lambda: \"e5-small-v2\"),\n",
        "    \"bge-small-en-v1.5\":      (\"dense_embed\", lambda: \"bge-small-en-v1.5\"),\n",
        "    \"distilroberta-base_CLS\": (\"dense_hf_cls\", lambda: \"distilroberta-base\"),\n",
        "}"
      ],
      "metadata": {
        "id": "5VgEk4jFW8mc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_eval_method(X_train_text, y_train, X_val_text, y_val, X_test_text, y_test, method_name: str):\n",
        "    mtype, factory = METHODS[method_name]\n",
        "    t0 = time.time()\n",
        "    timings = {}\n",
        "\n",
        "    if mtype in (\"sparse\",\"dense\"):\n",
        "        vectorizer_or_pipe = factory()\n",
        "        t_fit0 = time.time()\n",
        "        X_train = vectorizer_or_pipe.fit_transform(X_train_text)\n",
        "        timings[\"fit_vectorizer_s\"] = time.time() - t_fit0\n",
        "\n",
        "        t_tr0 = time.time()\n",
        "        X_val   = vectorizer_or_pipe.transform(X_val_text)\n",
        "        X_test  = vectorizer_or_pipe.transform(X_test_text)\n",
        "        timings[\"transform_s\"] = time.time() - t_tr0\n",
        "\n",
        "        # Choose classifier\n",
        "        if mtype == \"sparse\":\n",
        "            clf = LinearSVC()\n",
        "        else:  # dense\n",
        "            clf = LogisticRegression(max_iter=1000, n_jobs=-1, solver=\"lbfgs\")\n",
        "        t_clf0 = time.time()\n",
        "        clf.fit(X_train, y_train)\n",
        "        timings[\"fit_clf_s\"] = time.time() - t_clf0\n",
        "\n",
        "        # Evaluate\n",
        "        t_inf0 = time.time()\n",
        "        yhat = clf.predict(X_test)\n",
        "        timings[\"infer_s\"] = time.time() - t_inf0\n",
        "\n",
        "        acc = accuracy_score(y_test, yhat)\n",
        "        report = classification_report(y_test, yhat, output_dict=True)\n",
        "        return acc, report, timings\n",
        "\n",
        "    elif mtype == \"dense_embed\":\n",
        "        model_key = factory()\n",
        "        # cache for speed\n",
        "        cache_train = CACHE / f\"{method_name}_train.npy\"\n",
        "        cache_val   = CACHE / f\"{method_name}_val.npy\"\n",
        "        cache_test  = CACHE / f\"{method_name}_test.npy\"\n",
        "\n",
        "        if cache_train.exists() and cache_val.exists() and cache_test.exists():\n",
        "            X_train = np.load(cache_train); X_val = np.load(cache_val); X_test = np.load(cache_test)\n",
        "        else:\n",
        "            X_train = embed_with_sbert(model_key, X_train_text, batch_size=128)\n",
        "            X_val   = embed_with_sbert(model_key, X_val_text, batch_size=128)\n",
        "            X_test  = embed_with_sbert(model_key, X_test_text, batch_size=128)\n",
        "            np.save(cache_train, X_train); np.save(cache_val, X_val); np.save(cache_test, X_test)\n",
        "\n",
        "        clf = LogisticRegression(max_iter=1000, n_jobs=-1, solver=\"lbfgs\")\n",
        "        t_clf0 = time.time(); clf.fit(X_train, y_train); timings[\"fit_clf_s\"] = time.time() - t_clf0\n",
        "        t_inf0 = time.time(); yhat = clf.predict(X_test); timings[\"infer_s\"] = time.time() - t_inf0\n",
        "        acc = accuracy_score(y_test, yhat)\n",
        "        report = classification_report(y_test, yhat, output_dict=True)\n",
        "        return acc, report, timings\n",
        "\n",
        "    elif mtype == \"dense_hf_cls\":\n",
        "        hf_name = factory()\n",
        "        cache_train = CACHE / f\"{method_name}_train.npy\"\n",
        "        cache_val   = CACHE / f\"{method_name}_val.npy\"\n",
        "        cache_test  = CACHE / f\"{method_name}_test.npy\"\n",
        "\n",
        "        if cache_train.exists() and cache_val.exists() and cache_test.exists():\n",
        "            X_train = np.load(cache_train); X_val = np.load(cache_val); X_test = np.load(cache_test)\n",
        "        else:\n",
        "            X_train = embed_with_transformer_cls(hf_name, X_train_text, batch_size=64)\n",
        "            X_val   = embed_with_transformer_cls(hf_name, X_val_text, batch_size=64)\n",
        "            X_test  = embed_with_transformer_cls(hf_name, X_test_text, batch_size=64)\n",
        "            np.save(cache_train, X_train); np.save(cache_val, X_val); np.save(cache_test, X_test)\n",
        "\n",
        "        clf = LogisticRegression(max_iter=1000, n_jobs=-1, solver=\"lbfgs\")\n",
        "        t_clf0 = time.time(); clf.fit(X_train, y_train); timings[\"fit_clf_s\"] = time.time() - t_clf0\n",
        "        t_inf0 = time.time(); yhat = clf.predict(X_test); timings[\"infer_s\"] = time.time() - t_inf0\n",
        "        acc = accuracy_score(y_test, yhat)\n",
        "        report = classification_report(y_test, yhat, output_dict=True)\n",
        "        return acc, report, timings\n",
        "\n",
        "    else:\n",
        "        raise ValueError(mtype)"
      ],
      "metadata": {
        "id": "UF5uHcjLXDgn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ctc_pipeline(\n",
        "    reddit_path=\"/content/drive/MyDrive/CTC_by_source/CTC_Reddit_10k.json\",\n",
        "    stack_path=\"/content/drive/MyDrive/CTC_by_source/CTC_Stackexchange_10k.json\",\n",
        "    arxiv_path=\"/content/drive/MyDrive/CTC_by_source/CTC_arXiv_10k.json\",\n",
        "    combined_path=\"/content/drive/MyDrive/CTC_by_source/CTC_by_source_30k.json\",\n",
        "    epochs_dnn_per_source=8,\n",
        "    batch_size=512,\n",
        "    random_state=42,\n",
        "    test_size=0.20,\n",
        "    val_size_within_train=0.125,\n",
        "):\n",
        "    \"\"\"\n",
        "    One-call runner for the original CTC reproduction:\n",
        "      - Loads your per-source 10k JSONs + 30k combined\n",
        "      - Downloads authors' repo to get dictionary and validation folders\n",
        "      - Builds dictionary TF-IDF\n",
        "      - Trains 5 classic models + 2 DNN checkpoints per source (total 21 models)\n",
        "      - Evaluates on held-out test split + authors' validation folders\n",
        "      - Saves vectorizer + models in ./ctc_models\n",
        "    \"\"\"\n",
        "    import os, json, zipfile, requests, joblib, numpy as np\n",
        "    from pathlib import Path\n",
        "    from tqdm import tqdm\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score, classification_report\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.svm import LinearSVC\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    import re\n",
        "\n",
        "    # ---------- small helpers ----------\n",
        "    WORKDIR = Path(\"ctc_repro\"); WORKDIR.mkdir(exist_ok=True, parents=True)\n",
        "    MODELDIR = Path(\"ctc_models\"); MODELDIR.mkdir(exist_ok=True, parents=True)\n",
        "    DATA = WORKDIR / \"data\"; DATA.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    def stream_download(url: str, out_path: Path, desc: str = None):\n",
        "        if out_path.exists() and out_path.stat().st_size > 0:\n",
        "            return out_path\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            total = int(r.headers.get(\"content-length\", 0))\n",
        "            with open(out_path, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=desc or out_path.name) as pbar:\n",
        "                for chunk in r.iter_content(1024*1024):\n",
        "                    if chunk:\n",
        "                        f.write(chunk); pbar.update(len(chunk))\n",
        "        return out_path\n",
        "\n",
        "    # cleaning\n",
        "    CLEAN_HTML_RE = re.compile(r\"<[^>]+>\")\n",
        "    URL_RE = re.compile(r\"http[s]?://\\S+|www\\.\\S+\")\n",
        "    CODE_RE = re.compile(r\"`{1,3}.*?`{1,3}\", re.DOTALL)\n",
        "    NON_ASCII_RE = re.compile(r\"[^\\x00-\\x7F]+\")\n",
        "    WHITESPACE_RE = re.compile(r\"\\s+\")\n",
        "    def clean_text(s: str) -> str:\n",
        "        if not isinstance(s, str): return \"\"\n",
        "        s = URL_RE.sub(\" \", s)\n",
        "        s = CODE_RE.sub(\" \", s)\n",
        "        s = CLEAN_HTML_RE.sub(\" \", s)\n",
        "        s = NON_ASCII_RE.sub(\" \", s)\n",
        "        s = WHITESPACE_RE.sub(\" \", s).strip()\n",
        "        return s\n",
        "\n",
        "    def load_json_arr(path: Path):\n",
        "        data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "        X = [clean_text(d[\"text\"]) for d in data]\n",
        "        y = [int(d[\"label\"]) for d in data]\n",
        "        return X, y\n",
        "\n",
        "    def make_vectorizer_from_dictionary(dict_path: Path) -> TfidfVectorizer:\n",
        "        vocab = sorted({w.strip() for w in dict_path.read_text(\"utf-8\").splitlines() if w.strip()})\n",
        "        return TfidfVectorizer(\n",
        "            vocabulary=vocab,\n",
        "            lowercase=True,\n",
        "            dtype=np.float32,\n",
        "            token_pattern=r\"(?u)\\b\\w+\\b\",\n",
        "            max_df=1.0,\n",
        "            min_df=1,\n",
        "            norm=\"l2\",\n",
        "            use_idf=True,\n",
        "            smooth_idf=True,\n",
        "            sublinear_tf=True,\n",
        "        )\n",
        "\n",
        "    def build_classic_models():\n",
        "        return {\n",
        "            \"DecisionTree\": DecisionTreeClassifier(max_depth=100, random_state=random_state),\n",
        "            \"RandomForest\": RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=random_state),\n",
        "            \"Logistic\":     LogisticRegression(max_iter=500, n_jobs=-1, solver=\"saga\", penalty=\"l2\"),\n",
        "            \"LinearSVC\":    LinearSVC(),\n",
        "            \"MLP\":          MLPClassifier(hidden_layer_sizes=(256,), activation=\"relu\", max_iter=25, random_state=random_state),\n",
        "        }\n",
        "\n",
        "    def build_dnn(input_dim: int):\n",
        "        model = keras.Sequential([\n",
        "            layers.Input(shape=(input_dim,)),\n",
        "            layers.Dense(512, activation=\"relu\"),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(256, activation=\"relu\"),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(2, activation=\"softmax\"),\n",
        "        ])\n",
        "        model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    def pred_binary(model, X):\n",
        "        if hasattr(model, \"predict\"):\n",
        "            try:\n",
        "                return model.predict(X).astype(int)\n",
        "            except Exception:\n",
        "                pass\n",
        "        try:\n",
        "            df = model.decision_function(X)\n",
        "            return (df > 0).astype(int)\n",
        "        except Exception:\n",
        "            if hasattr(model, \"predict_proba\"):\n",
        "                proba = model.predict_proba(X)\n",
        "                return np.argmax(proba, axis=1).astype(int)\n",
        "            raise\n",
        "\n",
        "    def pred_binary_dnn(dnn, X):\n",
        "        P = dnn.predict(X.toarray(), verbose=0)\n",
        "        return np.argmax(P, axis=1).astype(int)\n",
        "\n",
        "    def majority_vote(preds_bin: list) -> np.ndarray:\n",
        "        stacked = np.vstack(preds_bin)\n",
        "        votes = stacked.sum(axis=0)\n",
        "        return (votes >= (stacked.shape[0]/2.0)).astype(int)\n",
        "\n",
        "    def ctc_predict(vec, models15: dict, dnns6: dict, X_text: list) -> np.ndarray:\n",
        "        X = vec.transform(X_text)\n",
        "        preds = [pred_binary(m, X) for m in models15.values()]\n",
        "        preds += [pred_binary_dnn(d, X) for d in dnns6.values()]\n",
        "        return majority_vote(preds)\n",
        "\n",
        "    # ---------- fetch authors' repo for dictionary + validation ----------\n",
        "    CTC_REPO_ZIP = \"https://codeload.github.com/epelofske-student/CTC/zip/refs/heads/main\"\n",
        "    DICT_REPO_PATH = \"English_word_dictionary.txt\"\n",
        "    VAL_DIR_CYB = \"validation_data_cybersecurity\"\n",
        "    VAL_DIR_NON = \"validation_data_non_cybersecurity\"\n",
        "\n",
        "    repo_zip = DATA / \"CTC-main.zip\"\n",
        "    repo_root = DATA / \"CTC-main\"\n",
        "    if not repo_root.exists():\n",
        "        stream_download(CTC_REPO_ZIP, repo_zip, \"CTC-main.zip\")\n",
        "        with zipfile.ZipFile(repo_zip, \"r\") as z:\n",
        "            z.extractall(DATA)\n",
        "\n",
        "    DICT_PATH = repo_root / DICT_REPO_PATH\n",
        "    VAL_CYB = repo_root / VAL_DIR_CYB\n",
        "    VAL_NON = repo_root / VAL_DIR_NON\n",
        "    assert DICT_PATH.exists(), \"Dictionary not found in repo\"\n",
        "    assert VAL_CYB.exists() and VAL_NON.exists(), \"Validation folders missing from repo\"\n",
        "\n",
        "    # ---------- load your data ----------\n",
        "    Rp, Sp, Ap, Cp = Path(reddit_path), Path(stack_path), Path(arxiv_path), Path(combined_path)\n",
        "    for p in [Rp, Sp, Ap, Cp]:\n",
        "        if not Path(p).exists():\n",
        "            raise FileNotFoundError(f\"Missing file: {p}\")\n",
        "\n",
        "    Xr, yr = load_json_arr(Rp)\n",
        "    Xs, ys = load_json_arr(Sp)\n",
        "    Xa, ya = load_json_arr(Ap)\n",
        "    Xall, yall = load_json_arr(Cp)\n",
        "\n",
        "    # combined split (train/val/test)\n",
        "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "        Xall, yall, test_size=test_size, random_state=random_state, stratify=yall\n",
        "    )\n",
        "    X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "        X_train_text, y_train, test_size=val_size_within_train, random_state=random_state, stratify=y_train\n",
        "    )\n",
        "\n",
        "    # ---------- vectorize with authors' dictionary ----------\n",
        "    vec = make_vectorizer_from_dictionary(DICT_PATH)\n",
        "    X_train = vec.fit_transform(X_train_text)\n",
        "    X_val   = vec.transform(X_val_text)\n",
        "    X_test  = vec.transform(X_test_text)\n",
        "    joblib.dump(vec, MODELDIR / \"tfidf_vectorizer.joblib\")\n",
        "    print(\"TF-IDF:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "\n",
        "    # ---------- per-source training (5 classics + 2 DNN) ----------\n",
        "    def train_source_bundle(source_name: str, X_text: list, y: list):\n",
        "        Xtr_txt, Xte_txt, ytr, yte = train_test_split(X_text, y, test_size=0.20, random_state=random_state, stratify=y)\n",
        "        Xtr_txt, Xva_txt, ytr, yva = train_test_split(Xtr_txt, ytr, test_size=0.125, random_state=random_state, stratify=ytr)\n",
        "        Xtr = vec.transform(Xtr_txt); Xva = vec.transform(Xva_txt); Xte = vec.transform(Xte_txt)\n",
        "\n",
        "        classics = build_classic_models()\n",
        "        trained = {}\n",
        "        for name, model in classics.items():\n",
        "            print(f\"[{source_name}] Training {name} ...\")\n",
        "            try:\n",
        "                if hasattr(model, \"class_weight\"):\n",
        "                    model.set_params(class_weight=\"balanced\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            model.fit(Xtr, ytr)\n",
        "            va_acc = accuracy_score(yva, model.predict(Xva))\n",
        "            print(f\"[{source_name}] {name} val acc: {va_acc:.4f}\")\n",
        "            joblib.dump(model, MODELDIR / f\"{source_name}_{name}.joblib\")\n",
        "            trained[f\"{source_name}_{name}\"] = model\n",
        "\n",
        "        print(f\"[{source_name}] Training DNN (checkpoints near ~0.95 and ~0.99 val acc) ...\")\n",
        "        dnn = build_dnn(Xtr.shape[1])\n",
        "        Xtr_d = Xtr.toarray(); Xva_d = Xva.toarray()\n",
        "        best = { \"DNN_t95\": (None, 1e9), \"DNN_t99\": (None, 1e9) }  # (model, gap)\n",
        "\n",
        "        targets = { \"DNN_t95\": 0.95, \"DNN_t99\": 0.99 }\n",
        "        for ep in range(1, epochs_dnn_per_source+1):\n",
        "            dnn.fit(Xtr_d, np.array(ytr), epochs=1, batch_size=batch_size, verbose=0)\n",
        "            _, va_acc = dnn.evaluate(Xva_d, np.array(yva), verbose=0)\n",
        "            print(f\"[{source_name}] DNN epoch {ep}: val acc={va_acc:.4f}\")\n",
        "            for tag, targ in targets.items():\n",
        "                gap = abs(va_acc - targ)\n",
        "                if gap < best[tag][1]:\n",
        "                    path = MODELDIR / f\"{source_name}_{tag}.keras\"\n",
        "                    dnn.save(path)\n",
        "                    best[tag] = (keras.models.load_model(path), gap)\n",
        "\n",
        "        dnns = { f\"{source_name}_DNN_t95\": best[\"DNN_t95\"][0],\n",
        "                 f\"{source_name}_DNN_t99\": best[\"DNN_t99\"][0] }\n",
        "        return trained, dnns, (Xte, yte)\n",
        "\n",
        "    sources = {\n",
        "        \"Reddit\": (Xr, yr),\n",
        "        \"Stackexchange\": (Xs, ys),\n",
        "        \"arXiv\": (Xa, ya),\n",
        "    }\n",
        "\n",
        "    all_classic, all_dnns, per_source_tests = {}, {}, {}\n",
        "    for sname, (Xt, yt) in sources.items():\n",
        "        mcls, mdnns, (Xte_src, yte_src) = train_source_bundle(sname, Xt, yt)\n",
        "        all_classic.update(mcls)\n",
        "        all_dnns.update(mdnns)\n",
        "        per_source_tests[sname] = (Xte_src, yte_src)\n",
        "\n",
        "    print(\"Classic models:\", len(all_classic), \"| DNNs:\", len(all_dnns))\n",
        "\n",
        "    # ---------- evaluate on your held-out combined TEST ----------\n",
        "    print(\"\\n=== Combined TEST split ===\")\n",
        "    yhat = ctc_predict(vec, all_classic, all_dnns, X_test_text)\n",
        "    acc = accuracy_score(y_test, yhat)\n",
        "    print(f\"CTC (21-model) TEST accuracy: {acc:.4f}\")\n",
        "    print(classification_report(y_test, yhat, target_names=[\"non-cybersecurity\",\"cybersecurity\"]))\n",
        "\n",
        "    # ---------- evaluate on authors' validation folders ----------\n",
        "    def read_text_dir(dir_path: Path, max_files=None):\n",
        "        files = sorted([p for p in dir_path.rglob(\"*\") if p.is_file()])\n",
        "        if max_files is not None: files = files[:max_files]\n",
        "        texts = []\n",
        "        for p in files:\n",
        "            try:\n",
        "                texts.append(clean_text(p.read_text(\"utf-8\", errors=\"ignore\")))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return texts\n",
        "\n",
        "    print(\"\\n=== Authors' original validation folders ===\")\n",
        "    X_cyb = read_text_dir(VAL_CYB)\n",
        "    X_non = read_text_dir(VAL_NON)\n",
        "\n",
        "    yhat_cyb = ctc_predict(vec, all_classic, all_dnns, X_cyb)\n",
        "    yhat_non = ctc_predict(vec, all_classic, all_dnns, X_non)\n",
        "\n",
        "    acc_cyb = (yhat_cyb == 1).mean()\n",
        "    acc_non = (yhat_non == 0).mean()\n",
        "    fp_rate = (yhat_non == 1).mean()\n",
        "    fn_rate = (yhat_cyb == 0).mean()\n",
        "\n",
        "    print(f\"Cybersecurity Val:      acc={acc_cyb:.4f}, FN={fn_rate:.4f}, N={len(X_cyb)}\")\n",
        "    print(f\"Non-cybersecurity Val:  acc={acc_non:.4f}, FP={fp_rate:.4f}, N={len(X_non)}\")\n",
        "\n",
        "    # ---------- save index ----------\n",
        "    index = {\n",
        "        \"vectorizer\": str((MODELDIR / \"tfidf_vectorizer.joblib\").resolve()),\n",
        "        \"classics\": sorted(list(all_classic.keys())),\n",
        "        \"dnns\": sorted(list(all_dnns.keys()))\n",
        "    }\n",
        "    (MODELDIR / \"ensemble_index.json\").write_text(json.dumps(index, indent=2), encoding=\"utf-8\")\n",
        "    print(\"\\nSaved ensemble index at:\", MODELDIR / \"ensemble_index.json\")\n",
        "    print(\"✅ Done.\")"
      ],
      "metadata": {
        "id": "IcmIZjdaXMYU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Mount + Locate + Run =====\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import os, glob, json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# 1) Try the common locations first\n",
        "candidates = [\n",
        "    \"/content/drive/MyDrive/CTC_by_source\",                 # standard My Drive\n",
        "]\n",
        "# 2) Also search recursively across all mounted Drive roots\n",
        "candidates += glob.glob(\"/content/drive/**/CTC_by_source\", recursive=True)\n",
        "\n",
        "# Deduplicate while preserving order\n",
        "seen = set(); found = []\n",
        "for p in candidates:\n",
        "    if p not in seen and os.path.isdir(p):\n",
        "        seen.add(p); found.append(p)\n",
        "\n",
        "if not found:\n",
        "    # Helpful listings so you can see where things are\n",
        "    print(\"⚠️ Could not find CTC_by_source automatically.\")\n",
        "    print(\"Contents of /content/drive:\", os.listdir(\"/content/drive\"))\n",
        "    if os.path.isdir(\"/content/drive/MyDrive\"):\n",
        "        print(\"Contents of /content/drive/MyDrive:\", os.listdir(\"/content/drive/MyDrive\"))\n",
        "    raise FileNotFoundError(\"CTC_by_source folder not found. If it’s in a Shared Drive, check under /content/drive/Shareddrives/<DriveName>/CTC_by_source\")\n",
        "\n",
        "BASE_DIR = Path(found[0])\n",
        "print(\"✅ Using folder:\", BASE_DIR)\n",
        "\n",
        "# --- Locate files inside the folder\n",
        "def first_match(base: Path, patterns):\n",
        "    for pat in patterns:\n",
        "        m = list(base.glob(pat))\n",
        "        if m: return m[0]\n",
        "    return None\n",
        "\n",
        "paths = {\n",
        "    \"reddit\":   first_match(BASE_DIR, [\"CTC_Reddit_10k.json\",\"*Reddit*10k*.json\",\"*reddit*10k*.json\"]),\n",
        "    \"stack\":    first_match(BASE_DIR, [\"CTC_Stackexchange_10k.json\",\"*Stack*10k*.json\",\"*stack*10k*.json\"]),\n",
        "    \"arxiv\":    first_match(BASE_DIR, [\"CTC_arXiv_10k.json\",\"*arXiv*10k*.json\",\"*arxiv*10k*.json\"]),\n",
        "    \"combined\": first_match(BASE_DIR, [\"CTC_by_source_30k.json\",\"*by_source*30k*.json\",\"*combined*30k*.json\"]),\n",
        "}\n",
        "print(\"\\nFound files:\")\n",
        "for k,v in paths.items(): print(f\"  {k:9s} -> {v}\")\n",
        "\n",
        "missing = [k for k,v in paths.items() if v is None or not Path(v).exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(f\"Missing required file(s): {missing}. Check names and that they’re in {BASE_DIR}\")\n",
        "\n",
        "# --- Tiny sanity check before running\n",
        "def sniff(p: Path, n=3):\n",
        "    data = json.loads(Path(p).read_text(encoding=\"utf-8\"))\n",
        "    labs = Counter(int(d[\"label\"]) for d in data if \"label\" in d)\n",
        "    print(f\"\\n{p.name}: {len(data)} items | labels {dict(labs)}\")\n",
        "    for r in data[:n]:\n",
        "        print(\"  -\", (r.get(\"text\",\"\")[:120] + (\"…\" if len(r.get(\"text\",\"\"))>120 else \"\")))\n",
        "\n",
        "for k in [\"reddit\",\"stack\",\"arxiv\",\"combined\"]:\n",
        "    sniff(Path(paths[k]))\n",
        "\n",
        "# --- Run the CTC pipeline with the detected paths\n",
        "# NOTE: run_ctc_pipeline() must already be defined in this notebook (from the previous message).\n",
        "res = run_ctc_pipeline(\n",
        "    reddit_path=str(paths[\"reddit\"]),\n",
        "    stack_path=str(paths[\"stack\"]),\n",
        "    arxiv_path=str(paths[\"arxiv\"]),\n",
        "    combined_path=str(paths[\"combined\"]),\n",
        "    epochs_dnn_per_source=6,   # tweak as you wish\n",
        "    batch_size=512,\n",
        "    random_state=42,\n",
        "    test_size=0.20,\n",
        "    val_size_within_train=0.125,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUtRRDyTYzDl",
        "outputId": "67434dfc-b087-4d3d-f377-512824e410a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Using folder: /content/drive/MyDrive/CTC_by_source\n",
            "\n",
            "Found files:\n",
            "  reddit    -> /content/drive/MyDrive/CTC_by_source/CTC_Reddit_10k.json\n",
            "  stack     -> /content/drive/MyDrive/CTC_by_source/CTC_Stackexchange_10k.json\n",
            "  arxiv     -> /content/drive/MyDrive/CTC_by_source/CTC_arXiv_10k.json\n",
            "  combined  -> /content/drive/MyDrive/CTC_by_source/CTC_by_source_30k.json\n",
            "\n",
            "CTC_Reddit_10k.json: 20000 items | labels {0: 10000, 1: 10000}\n",
            "  - Look both ways before crossing... ok, at least look one way... no, not that way.... \n",
            "  - Help: I need a cipher with a \"false bottom\". Hi! This is for a game, geocaching, so it doesn't need to be \"super secure\"…\n",
            "  - Computer Science (i.e., the science of computation; not programming) Discord Server. There are very few Discord servers …\n",
            "\n",
            "CTC_Stackexchange_10k.json: 20000 items | labels {1: 10000, 0: 10000}\n",
            "  - What is the most successful virus/rootkit?. <p>\"Successful\" is rated by infection rate. </p>\n",
            "\n",
            "<p>Which virus/rootkit/mal…\n",
            "  - How to fill voids (Nan) in Dataframe with mean values of that column?. <p>I have a dataframe with 4 columns: ch_name, ti…\n",
            "  - How to incorporate updated line colours into legend of a plot in R using lattice?. <h3>Context and Question:</h3>\n",
            "\n",
            "<p>I …\n",
            "\n",
            "CTC_arXiv_10k.json: 20000 items | labels {1: 10000, 0: 10000}\n",
            "  - \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Your Title Here\n",
            "\n",
            "\n",
            "HiFlash: A History Independent Flash Device\n",
            "\n",
            "Bo…\n",
            "  - \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "COMMENTARY ON: “Citing and Reading Behavours in High-\n",
            "Energy Physics…\n",
            "  - \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ar\n",
            "X\n",
            "\n",
            "iv\n",
            ":0\n",
            "\n",
            "70\n",
            "9.\n",
            "\n",
            "11\n",
            "73\n",
            "\n",
            "v1\n",
            "  [\n",
            "\n",
            "nl\n",
            "in\n",
            "\n",
            ".C\n",
            "G\n",
            "\n",
            "] \n",
            " 7\n",
            "\n",
            " S\n",
            "ep\n",
            "\n",
            " 2\n",
            "…\n",
            "\n",
            "CTC_by_source_30k.json: 60000 items | labels {0: 30000, 1: 30000}\n",
            "  - \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ar\n",
            "X\n",
            "\n",
            "iv\n",
            ":0\n",
            "\n",
            "70\n",
            "7.\n",
            "\n",
            "06\n",
            "52\n",
            "\n",
            "v1\n",
            "  [\n",
            "\n",
            "cs\n",
            ".N\n",
            "\n",
            "E\n",
            "] \n",
            "\n",
            " 4\n",
            " J\n",
            "\n",
            "ul\n",
            " 2\n",
            "\n",
            "00\n",
            "…\n",
            "  - Flutter ToggleButton Class - Flutter 1.9.1. <p>I am currently learning Flutter and making good progress so please bear w…\n",
            "  - Cryptopia technical Support phone number 1855-206-2326. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# CTC \"FAST\" Runner (no DNNs)\n",
        "# ===============================\n",
        "# - Trains 5 classic models per source (15 total), majority vote ensemble\n",
        "# - Optional per-source subsample to avoid long runtime\n",
        "# - Uses the original dictionary TF-IDF\n",
        "# - Saves models and prints metrics\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import os, json, zipfile, requests, joblib, re, time, random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# ----- CONFIG -----\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/CTC_by_source\")\n",
        "REDDIT = BASE_DIR / \"CTC_Reddit_10k.json\"\n",
        "STACK  = BASE_DIR / \"CTC_Stackexchange_10k.json\"\n",
        "ARXIV  = BASE_DIR / \"CTC_arXiv_10k.json\"\n",
        "COMBO  = BASE_DIR / \"CTC_by_source_30k.json\"\n",
        "\n",
        "# Speed knobs\n",
        "LIMIT_PER_SOURCE = 4000   # set to None for all 10k per source\n",
        "TEST_SIZE = 0.20\n",
        "VAL_SIZE_WITHIN_TRAIN = 0.125\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "WORKDIR = Path(\"ctc_fast\"); WORKDIR.mkdir(exist_ok=True, parents=True)\n",
        "MODELDIR = Path(\"ctc_models_fast\"); MODELDIR.mkdir(exist_ok=True, parents=True)\n",
        "DATA = WORKDIR / \"data\"; DATA.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Authors' repo for dictionary + validation (optional at the end)\n",
        "CTC_REPO_ZIP = \"https://codeload.github.com/epelofske-student/CTC/zip/refs/heads/main\"\n",
        "DICT_REPO_PATH = \"English_word_dictionary.txt\"\n",
        "VAL_DIR_CYB = \"validation_data_cybersecurity\"\n",
        "VAL_DIR_NON = \"validation_data_non_cybersecurity\"\n",
        "\n",
        "def stream_download(url: str, out_path: Path, desc: str = None):\n",
        "    if out_path.exists() and out_path.stat().st_size > 0:\n",
        "        return out_path\n",
        "    import requests\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        total = int(r.headers.get(\"content-length\", 0))\n",
        "        with open(out_path, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=desc or out_path.name) as pbar:\n",
        "            for chunk in r.iter_content(1024*1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk); pbar.update(len(chunk))\n",
        "    return out_path\n",
        "\n",
        "# Cleaning similar to earlier cells\n",
        "CLEAN_HTML_RE = re.compile(r\"<[^>]+>\")\n",
        "URL_RE = re.compile(r\"http[s]?://\\S+|www\\.\\S+\")\n",
        "CODE_RE = re.compile(r\"`{1,3}.*?`{1,3}\", re.DOTALL)\n",
        "NON_ASCII_RE = re.compile(r\"[^\\x00-\\x7F]+\")\n",
        "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
        "def clean_text(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = URL_RE.sub(\" \", s)\n",
        "    s = CODE_RE.sub(\" \", s)\n",
        "    s = CLEAN_HTML_RE.sub(\" \", s)\n",
        "    s = NON_ASCII_RE.sub(\" \", s)\n",
        "    s = WHITESPACE_RE.sub(\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def load_json_arr(path: Path, limit=None, seed=RANDOM_STATE):\n",
        "    data = json.loads(path.read_text(encoding=\"utf-8\"))\n",
        "    if limit is not None and len(data) > limit:\n",
        "        random.Random(seed).shuffle(data)\n",
        "        data = data[:limit]\n",
        "    X = [clean_text(d[\"text\"]) for d in data]\n",
        "    y = [int(d[\"label\"]) for d in data]\n",
        "    return X, y\n",
        "\n",
        "# 1) Fetch dictionary\n",
        "repo_zip = DATA / \"CTC-main.zip\"\n",
        "repo_root = DATA / \"CTC-main\"\n",
        "if not repo_root.exists():\n",
        "    stream_download(CTC_REPO_ZIP, repo_zip, \"CTC-main.zip\")\n",
        "    with zipfile.ZipFile(repo_zip, \"r\") as z:\n",
        "        z.extractall(DATA)\n",
        "DICT_PATH = repo_root / DICT_REPO_PATH\n",
        "assert DICT_PATH.exists(), \"Dictionary not found in repo\"\n",
        "\n",
        "# 2) Load your data (optionally subsample for speed)\n",
        "print(\"Loading data…\")\n",
        "Xr, yr = load_json_arr(REDDIT, limit=LIMIT_PER_SOURCE)\n",
        "Xs, ys = load_json_arr(STACK,  limit=LIMIT_PER_SOURCE)\n",
        "Xa, ya = load_json_arr(ARXIV,  limit=LIMIT_PER_SOURCE)\n",
        "\n",
        "Xall, yall = load_json_arr(COMBO, limit=(None if LIMIT_PER_SOURCE is None else 3*LIMIT_PER_SOURCE))\n",
        "\n",
        "# 3) Build dictionary TF-IDF and combined splits\n",
        "vocab = sorted({w.strip() for w in DICT_PATH.read_text(\"utf-8\").splitlines() if w.strip()})\n",
        "vec = TfidfVectorizer(vocabulary=vocab, lowercase=True, dtype=np.float32,\n",
        "                      token_pattern=r\"(?u)\\b\\w+\\b\", norm=\"l2\", use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    Xall, yall, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=yall\n",
        ")\n",
        "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "    X_train_text, y_train, test_size=VAL_SIZE_WITHIN_TRAIN, random_state=RANDOM_STATE, stratify=y_train\n",
        ")\n",
        "X_train = vec.fit_transform(X_train_text)\n",
        "X_val   = vec.transform(X_val_text)\n",
        "X_test  = vec.transform(X_test_text)\n",
        "joblib.dump(vec, MODELDIR / \"tfidf_vectorizer.joblib\")\n",
        "print(\"TF-IDF shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "\n",
        "# 4) Classic models per source\n",
        "def build_classic_models():\n",
        "    return {\n",
        "        \"DecisionTree\": DecisionTreeClassifier(max_depth=100, random_state=RANDOM_STATE),\n",
        "        \"RandomForest\": RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=RANDOM_STATE),\n",
        "        \"Logistic\":     LogisticRegression(max_iter=500, n_jobs=-1, solver=\"saga\", penalty=\"l2\"),\n",
        "        \"LinearSVC\":    LinearSVC(),\n",
        "        \"MLP\":          MLPClassifier(hidden_layer_sizes=(256,), activation=\"relu\", max_iter=25, random_state=RANDOM_STATE),\n",
        "    }\n",
        "\n",
        "def train_source_classics(source_name: str, X_text, y):\n",
        "    Xtr_txt, Xte_txt, ytr, yte = train_test_split(X_text, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y)\n",
        "    Xtr_txt, Xva_txt, ytr, yva = train_test_split(Xtr_txt, ytr, test_size=0.125, random_state=RANDOM_STATE, stratify=ytr)\n",
        "    Xtr = vec.transform(Xtr_txt); Xva = vec.transform(Xva_txt); Xte = vec.transform(Xte_txt)\n",
        "\n",
        "    classics = build_classic_models()\n",
        "    trained = {}\n",
        "    for name, model in classics.items():\n",
        "        print(f\"[{source_name}] Training {name} …\")\n",
        "        try:\n",
        "            if hasattr(model, \"class_weight\"): model.set_params(class_weight=\"balanced\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        model.fit(Xtr, ytr)\n",
        "        va_acc = accuracy_score(yva, model.predict(Xva))\n",
        "        print(f\"[{source_name}] {name} val acc: {va_acc:.4f}\")\n",
        "        joblib.dump(model, MODELDIR / f\"{source_name}_{name}.joblib\")\n",
        "        trained[f\"{source_name}_{name}\"] = model\n",
        "    return trained, (Xte, yte)\n",
        "\n",
        "def pred_binary(model, X):\n",
        "    if hasattr(model, \"predict\"):\n",
        "        try:\n",
        "            return model.predict(X).astype(int)\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        df = model.decision_function(X)\n",
        "        return (df > 0).astype(int)\n",
        "    except Exception:\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            proba = model.predict_proba(X)\n",
        "            return np.argmax(proba, axis=1).astype(int)\n",
        "        raise\n",
        "\n",
        "def majority_vote(preds_bin: list) -> np.ndarray:\n",
        "    stacked = np.vstack(preds_bin)\n",
        "    votes = stacked.sum(axis=0)\n",
        "    return (votes >= (stacked.shape[0]/2.0)).astype(int)\n",
        "\n",
        "def ctc_predict(vec, models15: dict, X_text: list) -> np.ndarray:\n",
        "    X = vec.transform(X_text)\n",
        "    preds = [pred_binary(m, X) for m in models15.values()]\n",
        "    return majority_vote(preds)\n",
        "\n",
        "sources = {\n",
        "    \"Reddit\": (Xr, yr),\n",
        "    \"Stackexchange\": (Xs, ys),\n",
        "    \"arXiv\": (Xa, ya),\n",
        "}\n",
        "\n",
        "all_models = {}\n",
        "per_source_tests = {}\n",
        "for sname, (Xt, yt) in sources.items():\n",
        "    mcls, (Xte_src, yte_src) = train_source_classics(sname, Xt, yt)\n",
        "    all_models.update(mcls)\n",
        "    per_source_tests[sname] = (Xte_src, yte_src)\n",
        "\n",
        "print(\"\\nClassic models trained:\", len(all_models))  # should be 15\n",
        "\n",
        "# 5) Evaluate ensemble on your combined TEST split\n",
        "print(\"\\n=== Combined TEST split (FAST) ===\")\n",
        "yhat = ctc_predict(vec, all_models, X_test_text)\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print(f\"CTC (15 classic) TEST accuracy: {acc:.4f}\")\n",
        "print(classification_report(y_test, yhat, target_names=[\"non-cybersecurity\",\"cybersecurity\"]))\n",
        "\n",
        "print(\"\\n✅ FAST run complete. Models saved in:\", MODELDIR.resolve())"
      ],
      "metadata": {
        "id": "cNmiGI-TigJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Per-Source DNN Trainer (resume-safe)\n",
        "# ============================================\n",
        "\n",
        "# --- Config you can tweak ---\n",
        "BASE_DIR = \"/content/drive/MyDrive/CTC_by_source\"     # where your 10k/source JSONs live\n",
        "MODEL_DIR = \"ctc_models\"                               # where vectorizer & classic models live\n",
        "OUT_DIR   = \"ctc_dnn_checkpoints\"                      # where DNN checkpoints will be saved\n",
        "\n",
        "EPOCHS_PER_CALL = 4        # how many epochs to run in THIS invocation (safe to call multiple times)\n",
        "BATCH_SIZE = 512\n",
        "RANDOM_STATE = 42\n",
        "TARGETS = {\"DNN_t95\": 0.95, \"DNN_t99\": 0.99}  # we keep checkpoints closest to these val accuracies\n",
        "\n",
        "# Optional cap for very small VRAM; set to None to use all data from the 10k file\n",
        "LIMIT_PER_SOURCE = None    # e.g., 6000 to speed up quick runs\n",
        "\n",
        "# -------------------------------------------\n",
        "# Imports\n",
        "import os, json, re, zipfile, requests, joblib, glob\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Colab niceties\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "try:\n",
        "    for g in tf.config.list_physical_devices(\"GPU\"):\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# -------------------------------------------\n",
        "# Helpers\n",
        "\n",
        "CLEAN_HTML_RE = re.compile(r\"<[^>]+>\")\n",
        "URL_RE = re.compile(r\"http[s]?://\\S+|www\\.\\S+\")\n",
        "CODE_RE = re.compile(r\"`{1,3}.*?`{1,3}\", re.DOTALL)\n",
        "NON_ASCII_RE = re.compile(r\"[^\\x00-\\x7F]+\")\n",
        "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = URL_RE.sub(\" \", s)\n",
        "    s = CODE_RE.sub(\" \", s)\n",
        "    s = CLEAN_HTML_RE.sub(\" \", s)\n",
        "    s = NON_ASCII_RE.sub(\" \", s)\n",
        "    s = WHITESPACE_RE.sub(\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def load_ctc_json(p: Path, limit=None, seed=RANDOM_STATE):\n",
        "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "    if limit and len(data) > limit:\n",
        "        rng = np.random.default_rng(seed); idx = rng.permutation(len(data))[:limit]\n",
        "        data = [data[i] for i in idx]\n",
        "    X = [clean_text(d[\"text\"]) for d in data]\n",
        "    y = [int(d[\"label\"]) for d in data]\n",
        "    return X, y\n",
        "\n",
        "def build_dnn(input_dim: int):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(512, activation=\"relu\"),\n",
        "        layers.Dropout(0.30),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.Dropout(0.20),\n",
        "        layers.Dense(2, activation=\"softmax\"),\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def latest_epoch_ckpt(ckpt_dir: Path):\n",
        "    files = sorted(ckpt_dir.glob(\"epoch_*.keras\"))\n",
        "    if not files:\n",
        "        return None\n",
        "    return files[-1]\n",
        "\n",
        "def save_meta(meta_path: Path, meta: dict):\n",
        "    meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "def load_meta(meta_path: Path):\n",
        "    if meta_path.exists():\n",
        "        try:\n",
        "            return json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    # default meta\n",
        "    return {\n",
        "        \"best\": { \"DNN_t95\": {\"acc\": 0.0, \"gap\": 1e9, \"path\": None},\n",
        "                  \"DNN_t99\": {\"acc\": 0.0, \"gap\": 1e9, \"path\": None}},\n",
        "        \"epochs_trained\": 0\n",
        "    }\n",
        "\n",
        "def get_source_paths(source_name: str) -> Path:\n",
        "    base = Path(BASE_DIR)\n",
        "    mapping = {\n",
        "        \"Reddit\":       \"CTC_Reddit_10k.json\",\n",
        "        \"Stackexchange\":\"CTC_Stackexchange_10k.json\",\n",
        "        \"arXiv\":        \"CTC_arXiv_10k.json\",\n",
        "    }\n",
        "    fn = mapping.get(source_name)\n",
        "    if not fn:\n",
        "        raise ValueError(\"source_name must be one of: 'Reddit', 'Stackexchange', 'arXiv'\")\n",
        "    p = base / fn\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Not found: {p}\")\n",
        "    return p\n",
        "\n",
        "def load_vectorizer_or_fail():\n",
        "    vec_path = Path(MODEL_DIR) / \"tfidf_vectorizer.joblib\"\n",
        "    if not vec_path.exists():\n",
        "        raise FileNotFoundError(f\"Can't find vectorizer at {vec_path}. \"\n",
        "                                f\"Run the FAST runner or the full pipeline once to create it.\")\n",
        "    return joblib.load(vec_path)\n",
        "\n",
        "# -------------------------------------------\n",
        "# Main callable\n",
        "\n",
        "def train_dnn_source(source_name: str,\n",
        "                     epochs_this_call: int = EPOCHS_PER_CALL,\n",
        "                     batch_size: int = BATCH_SIZE,\n",
        "                     limit_per_source = LIMIT_PER_SOURCE):\n",
        "    \"\"\"\n",
        "    Train/Resume DNN for a single source.\n",
        "    Splits that source 80/20 then 12.5% of train as val (≈10% of original).\n",
        "    Saves:\n",
        "      - per-epoch checkpoints: {OUT_DIR}/{source}/epoch_{N}.keras\n",
        "      - best-by-target:        {OUT_DIR}/{source}/{source}_DNN_t95.keras and _t99.keras\n",
        "      - meta.json with best accuracies & epochs_trained\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== DNN training for source: {source_name} ===\")\n",
        "    src_path = get_source_paths(source_name)\n",
        "    vec = load_vectorizer_or_fail()\n",
        "\n",
        "    # Load data for this source\n",
        "    X, y = load_ctc_json(src_path, limit=limit_per_source)\n",
        "    Xtr_txt, Xte_txt, ytr, yte = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y)\n",
        "    Xtr_txt, Xva_txt, ytr, yva = train_test_split(Xtr_txt, ytr, test_size=0.125, random_state=RANDOM_STATE, stratify=ytr)\n",
        "\n",
        "    # Vectorize and densify (DNN needs dense)\n",
        "    Xtr = vec.transform(Xtr_txt).toarray()\n",
        "    Xva = vec.transform(Xva_txt).toarray()\n",
        "\n",
        "    # Prepare dirs\n",
        "    out_root = Path(OUT_DIR); out_root.mkdir(parents=True, exist_ok=True)\n",
        "    ckpt_dir = out_root / source_name; ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "    meta_path = ckpt_dir / \"meta.json\"\n",
        "    meta = load_meta(meta_path)\n",
        "\n",
        "    # Create or resume model\n",
        "    latest = latest_epoch_ckpt(ckpt_dir)\n",
        "    if latest:\n",
        "        print(f\"Resuming from latest checkpoint: {latest.name}\")\n",
        "        dnn = keras.models.load_model(latest)\n",
        "        start_epoch = int(latest.stem.split(\"_\")[-1]) + 1\n",
        "    else:\n",
        "        print(\"No prior checkpoint — starting fresh.\")\n",
        "        dnn = build_dnn(Xtr.shape[1])\n",
        "        start_epoch = 1\n",
        "\n",
        "    # Train a few epochs this call\n",
        "    for ep in range(start_epoch, start_epoch + epochs_this_call):\n",
        "        dnn.fit(Xtr, np.array(ytr), epochs=1, batch_size=batch_size, verbose=1)\n",
        "        va_loss, va_acc = dnn.evaluate(Xva, np.array(yva), verbose=0)\n",
        "        print(f\"[{source_name}] epoch {ep} — val acc: {va_acc:.4f}\")\n",
        "\n",
        "        # Save epoch checkpoint\n",
        "        ep_path = ckpt_dir / f\"epoch_{ep:03d}.keras\"\n",
        "        dnn.save(ep_path)\n",
        "\n",
        "        # Update best-by-target checkpoints\n",
        "        for tag, targ in TARGETS.items():\n",
        "            gap = abs(float(va_acc) - targ)\n",
        "            if gap < meta[\"best\"][tag][\"gap\"]:\n",
        "                best_path = ckpt_dir / f\"{source_name}_{tag}.keras\"\n",
        "                dnn.save(best_path)\n",
        "                meta[\"best\"][tag] = {\"acc\": float(va_acc), \"gap\": float(gap), \"path\": str(best_path)}\n",
        "                print(f\"  ↳ Updated {tag} ({targ:.2f}) → acc={va_acc:.4f}, saved: {best_path.name}\")\n",
        "\n",
        "        meta[\"epochs_trained\"] = ep\n",
        "        save_meta(meta_path, meta)\n",
        "\n",
        "    print(\"\\n✅ Done for\", source_name)\n",
        "    print(\"Best snapshots so far:\")\n",
        "    for tag, info in meta[\"best\"].items():\n",
        "        print(f\"  {tag}: acc≈{info['acc']:.4f}, path={info['path']}\")\n",
        "    print(f\"All checkpoints in: {ckpt_dir.resolve()}\")"
      ],
      "metadata": {
        "id": "cFx-_mEAiVCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F56sM-KUic75"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}